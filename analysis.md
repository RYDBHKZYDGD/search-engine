## COSC 488 - Information Retrieval 
## Search Engine Project Analysis

The index with the smallest lexicon was the stem index, which had ~30,000 terms. This is because the Porter Stemmer reduces variations of each word and create a common stem. According to the Pre-Processing lecture slides, stemming reduces the term index by ~17%. This is consistent with my results, because my single term index lexicon was 36,637 terms and 36,637 * .83 = ~30,000 terms. The index with the largest number of terms was the phrase index with ~167,405 terms. This was due to the larger number of special characters that exist in the file that the nltk tokenizer did not split on. After filtering the phrase index to only include terms with a document frequency of one or more, the lexicon size went down to ~34,000, which is comparable with the other indices’ lexicon sizes. The single term positional index contains more terms than the single term index because the positional index includes stop words. Additionally, special token normalization was performed for the single term index. Therefore, tokens like date were standardized. This makes it more likely that tokens map to the same term. 

The average index size was 11.12 MB. The single term positional index (20.8 MB) was twice as large as the single term index (10.3 MB) because the positional index stores not only each term’s document frequency, but also each term’s position for each document. In retrospect, I could have optimized the index sizes is by adding less formatting. The format of each index is term -> [[doc1, df], [doc2, df], ... ].  I could have saved space by removing the “ -> “ and the square brackets. 

The maximum document frequencies of the single and stem indices were 1,251 and 1,202, both for the term “1”. For the phrase index, the max df was 564 for the phrase “billing code.” The max document frequency for the single term positional index was 1,742 for the term “of,” which is a stop word. This indicates that stop words can occur many times in a collection and are often not discriminating or useful. 

The mean document frequency for the single term, stem, and positional index were all around 10. Because the median document frequency for all indices is 1, I infer that the distribution of document frequencies across all terms would be skewed right. 

Across all the indices, the fewer the temporary files (and larger the triple size), the faster the running time. This is because fewer temporary files means less overhead in the form of read, write, sort, and merge operations. Across all indices and memory constraints, the time taken to merge temp files was under 5s, which makes it a pretty cheap operation. The positional index running times was the fastest because it requires the least pre-processing. The single term index runtimes were several seconds longer because every file line had to be checked for special tokens using regular expressions and processed accordingly. The stem index took around 10s longer than the single and positional, perhaps because each token had to be processed by a stemmer object.  Finally, the phrase index took the longest, possibly because of the additional logic in the preProcess method, larger lexicon size, and additional filtered index creation. 
