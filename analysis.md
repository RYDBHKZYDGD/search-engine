## COSC 488 - Information Retrieval 
## Search Engine Project Analysis

The index with the smallest lexicon was the stem index, which had ~30,000 terms. This is because the Porter Stemmer reduces variations of each word and create a common stem. According to the Pre-Processing lecture slides, stemming reduces the term index by ~17%. This is consistent with my results, because my single term index lexicon was 36,637 terms and 36,637 * .83 = ~30,000 terms. The index with the largest number of terms was the phrase index with ~167,405 terms. This was due to the larger number of special characters that exist in the file that the nltk tokenizer did not split on. After filtering the phrase index to only include terms with a document frequency of one or more, the lexicon size went down to ~34,000, which is comparable with the other indices’ lexicon sizes. The single term positional index contains more terms than the single term index because the positional index includes stop words. Additionally, special token normalization was performed for the single term index. Therefore, tokens like date were standardized. This makes it more likely that tokens map to the same term. 

The average index size was 11.12 MB. The single term positional index (20.8 MB) was twice as large as the single term index (10.3 MB) because the positional index stores not only each term’s document frequency, but also each term’s position for each document. In retrospect, I could have optimized the index sizes is by adding less formatting. The format of each index is term -> [[doc1, df], [doc2, df], ... ].  I could have saved space by removing the “ -> “ and the square brackets. 

The maximum document frequencies of the single and stem indices were 1,251 and 1,202, both for the term “1”. For the phrase index, the max df was 564 for the phrase “billing code.” The max document frequency for the single term positional index was 1,742 for the term “of,” which is a stop word. This indicates that stop words can occur many times in a collection and are often not discriminating or useful. 

The mean document frequency for the single term, stem, and positional index were all around 10. Because the median document frequency for all indices is 1, I infer that the distribution of document frequencies across all terms would be skewed right. 

Across all the indices, the fewer the temporary files (and larger the triple size), the faster the running time. This is because fewer temporary files means less overhead in the form of read, write, sort, and merge operations. Across all indices and memory constraints, the time taken to merge temp files was under 5s, which makes it a pretty cheap operation. The positional index running times was the fastest because it requires the least pre-processing. The single term index runtimes were several seconds longer because every file line had to be checked for special tokens using regular expressions and processed accordingly. The stem index took around 10s longer than the single and positional, perhaps because each token had to be processed by a stemmer object.  Finally, the phrase index took the longest, possibly because of the additional logic in the preProcess method, larger lexicon size, and additional filtered index creation. 


According to the Language Model powerpoint, Query likelihood with Dirichlet smoothing offers similar performance to TF-IDF & BM25 retrieval functions. My findings correspond to this statement. The MAP for all models was around 30-40%. The MAP of the single and stem index was better for BM25 and LM than Cosine by around 10%. This may be because a problem with the Cosine similarity measure is that longer documents are somewhat penalized although they indeed they might have more components that are actually relevant. Between the stem and single term index, the stem index had a slightly higher MAP. This is evidence that stemming improves effectiveness by providing a better match between query and a relevant document. 

My engine’s query processing time was shortest for Cosine and longest for LM. This may be because in addition to calculating similarity scores with documents with tf > 0, Query likelihood with Dirichlet smoothing also assigns probabilities to unseen terms in document. This is important because it aims to fix the estimation (score = 0 if a term is missing in document) and data sparsity (document may be relevant to query but the query term is absent from document) problems. This required more computation, thereby increasing the run time. The query processing time for the stem index relative to the single term index was slightly faster for my engine for but slightly slower for ElasticSearch’s. Mine was faster most likely because my stem index is smaller than my single term index. 

Across the board, ElasticSearch’s query processing times were significantly faster than mine. This is because of ElasticSearch operates in a distributed environment. It partitions its index into different shards stored on a set of clusters. This allows queries to be processed in parallel, which is much more efficient. 

My dynamic query processing program performed slightly better than my static query processing program did. The reason why it didn’t have a significant impact could be because a relatively small number of phrases were sent to the phrase and positional indexes, and because a small number of documents were retrieved by these indexes. Specifically, phrases were sent to the phrase index only 11 times and to the positional index 6 times. Additionally, different indexes have different values for df, tf, cf, idf, and C, which could affect the similarity score calculation. 
